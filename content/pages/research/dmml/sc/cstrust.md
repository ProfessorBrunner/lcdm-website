Title: Quantifying the Reliability of Crowdsourcing classifications.
Slug: cstrust
Date: 2014-02-10
Modified: 
Authors: Robert J. Brunner
Save_as: research/dmml/sc/cstrust.html
Tags: Data Mining, Machine Learning, Classification, Image Classification, Crowdsourcing
Type: subproject
Parent: sc
Construction: True
Summary: We are exploring techniques to quantify the reliability of human classification volunteers.

Despite intensive effort on different computing approaches, the human
mind remains the best classification engine for a variety of projects.
As a result, a number of domains have employed crowdsourcing to generate
large samples iof classified data. This model first trains human
volunteers in a basic classification task, has the volunteers classify a
number of sources, and subsequently aggregates their classifications to
generate a final list of classified data. In the end, the results from
multiple, independent classifiers can be combined to generate a more
reliable classifications, in a similar manner as [ensemble learning]().
A secondary benefit of crowdsourcing is the strong educational component
that results naturally from employing many citizen scientists within a
research effort.

One aspect that remains unexplored with this technique, however, is the
likelihood that different individuals will have different abilities for
the same task. In collaboration with the [Image Formation and Processing
group](http://beckman.illinois.edu/research/themes/hcii/image-formation-
and-processing) at the [Beckman Institute](http://beckman.illinois.edu),
we are exploring the reliability of human volunteers in the
classification of astrophysical sources.

## Data

The primary application area is the results generated by the
[GalaxyZoo](http://www.galaxyzoo.org) project. This project has
developed an impressive infrastructure to support the classification of
different scientific data by very large numbers of citizen scientists.
The results from this project include publications and subsequent
scientific analysis of the classified data. We have requested access,
however, to the anonymzed classifications in order to ascertain if a
reliability rating can be developed to improve the overall efficacy of
the human classifications.

A second project is the [Dark Energy
Survey](http://eyeball.erinsheldon.net/index.html) exposure checker, a
tool developed by Peter Melchior and Erin Sheldon to allow DES members
to visually inspect DES images for defects as well as visually
interesting phenomena. We hope to aid this project in developing
infrastructure to gather sufficient training data to better quantify the
reliability component.

## Personnel

The lead student on this project is Xianming Liu.


